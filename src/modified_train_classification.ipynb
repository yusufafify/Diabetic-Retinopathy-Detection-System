{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca667f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11dcbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMG_DIR = Path(\"data/processed/classification/images\")\n",
    "LABEL_DIR = Path(\"data/processed/classification/labels\")\n",
    "MODEL_PATH = Path(\"models/classification_model.pth\")\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45338972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Custom Dataset\n",
    "class FundusDataset(Dataset):\n",
    "    def __init__(self, img_files, transform=None):\n",
    "        self.img_files = img_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        label_path = LABEL_DIR / (img_path.stem + \".txt\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(open(label_path).read().strip())\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a27dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Loaders (updated with augmentation)\n",
    "def get_dataloaders():\n",
    "    all_images = list(IMG_DIR.glob(\"*.jpg\"))\n",
    "    valid_images = []\n",
    "    for img_path in all_images:\n",
    "        label_path = LABEL_DIR / f\"{img_path.stem}.txt\"\n",
    "        if label_path.exists():\n",
    "            valid_images.append(img_path)\n",
    "        else:\n",
    "            print(f\"Missing label for: {img_path.name}\")\n",
    "\n",
    "    if len(valid_images) < 2:\n",
    "        raise ValueError(\"Not enough valid samples for splitting!\")\n",
    "\n",
    "    # Split into train (60%), val (20%), test (20%)\n",
    "    train_files, test_val_files = train_test_split(valid_images, test_size=0.4, random_state=42)\n",
    "    val_files, test_files = train_test_split(test_val_files, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Augmentation for training data\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    # Simple transform for validation/test\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    train_ds = FundusDataset(train_files, train_transform)\n",
    "    val_ds = FundusDataset(val_files, val_transform)\n",
    "    test_ds = FundusDataset(test_files, val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805869db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MixUp implementation\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha=0.4):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc29925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training Loop (with regularization)\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader, val_loader, _ = get_dataloaders()\n",
    "\n",
    "    # Model with dropout\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),  # 50% dropout\n",
    "        nn.Linear(model.fc.in_features, 5)\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss function with class weighting (adjust weights according to your data)\n",
    "    class_weights = torch.tensor([1.0, 2.0, 2.0, 3.0, 3.0])  # Example weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, correct = 0.0, 0\n",
    "\n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            # Save best model\n",
    "            MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete. Best model saved to:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6f503ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 1.5944, Train Acc: 0.3320, Val Loss: 1.2115, Val Acc: 0.6265\n",
      "Epoch 2/30, Train Loss: 1.2281, Train Acc: 0.4899, Val Loss: 1.0794, Val Acc: 0.5542\n",
      "Epoch 3/30, Train Loss: 1.0619, Train Acc: 0.5789, Val Loss: 1.1183, Val Acc: 0.5422\n",
      "Epoch 4/30, Train Loss: 0.9319, Train Acc: 0.6275, Val Loss: 0.9880, Val Acc: 0.6506\n",
      "Epoch 5/30, Train Loss: 0.7433, Train Acc: 0.7206, Val Loss: 1.1108, Val Acc: 0.6386\n",
      "Epoch 6/30, Train Loss: 0.6591, Train Acc: 0.7611, Val Loss: 1.0784, Val Acc: 0.6024\n",
      "Early stopping at epoch 7\n",
      "Training complete. Best model saved to: models\\classification_model.pth\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
